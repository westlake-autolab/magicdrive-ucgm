#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸¦æ–¹å·®åˆ†æçš„MagicDrive-V2æ¨ç†è„šæœ¬

è¿™æ˜¯ä¸€ä¸ªä¿®æ”¹ç‰ˆçš„æ¨ç†è„šæœ¬ï¼Œé›†æˆäº†xå‚æ•°æ–¹å·®åˆ†æåŠŸèƒ½ã€‚
åŸºäºåŸå§‹çš„ scripts/inference_magicdrive.pyï¼Œæ·»åŠ äº†æ–¹å·®åˆ†æè¡¥ä¸ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
torchrun --standalone --nproc_per_node ${GPUS} inference_with_variance_analysis.py ${CFG} \
    --cfg-options model.from_pretrained=${PATH_TO_MODEL} num_frames=${FRAME} \
    cpu_offload=true scheduler.type=rflow-slice
"""

import argparse
import os
import sys
from datetime import timedelta
from pathlib import Path

import colossalai
import torch
import torch.distributed as dist
from mmengine.runner import set_random_seed
from omegaconf import OmegaConf
from torch.utils.data import DataLoader
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥MagicDriveç›¸å…³æ¨¡å—
from magicdrivedit.acceleration.parallel_states import (
    get_data_parallel_group,
    get_sequence_parallel_group,
    get_sequence_parallel_rank,
    get_sequence_parallel_world_size,
    is_distributed,
    is_main_process,
)
from magicdrivedit.datasets import DATASETS
from magicdrivedit.models import MODELS
from magicdrivedit.registry import build_module
from magicdrivedit.schedulers import SCHEDULERS
from magicdrivedit.utils.config_utils import parse_configs
from magicdrivedit.utils.misc import (
    all_reduce_mean,
    create_logger,
    format_numel_str,
    get_model_numel,
    requires_grad,
    to_torch_dtype,
)

# å¯¼å…¥æ–¹å·®åˆ†æè¡¥ä¸
from variance_analysis_patch import apply_variance_analysis_patch


def main():
    # =============================
    # 1. é…ç½®è§£æå’Œç¯å¢ƒè®¾ç½®
    # =============================
    cfg = parse_configs(training=False)
    print("Configuration loaded successfully!")
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(seed=cfg.get("seed", 1024))
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹é…ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = to_torch_dtype(cfg.get("dtype", "fp16"))
    
    print(f"Using device: {device}, dtype: {dtype}")
    
    # =============================
    # 2. åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–
    # =============================
    if is_distributed():
        dist.init_process_group(backend="nccl", timeout=timedelta(hours=1))
        torch.cuda.set_device(dist.get_rank() % torch.cuda.device_count())
        colossalai.launch_from_torch({})
    
    # =============================
    # 3. æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨æ„å»º
    # =============================
    print("Building dataset...")
    
    # æ•°æ®é›†é…ç½®
    dataset_cfg = cfg.dataset
    if hasattr(dataset_cfg, "img_collate_param"):
        dataset_cfg.img_collate_param.is_train = False
    
    # æ„å»ºæ•°æ®é›†
    dataset = build_module(dataset_cfg, DATASETS)
    
    # éªŒè¯ç´¢å¼•å¤„ç†
    val_indices = cfg.get("val_indices", "all")
    if val_indices == "even":
        dataset.samples = [dataset.samples[i] for i in range(0, len(dataset.samples), 2)]
    elif val_indices == "odd":
        dataset.samples = [dataset.samples[i] for i in range(1, len(dataset.samples), 2)]
    
    print(f"Dataset size: {len(dataset)}")
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=1,  # æ¨ç†æ—¶é€šå¸¸ä½¿ç”¨batch_size=1
        shuffle=False,
        num_workers=cfg.get("num_workers", 4),
        pin_memory=True,
        drop_last=False,
    )
    
    # =============================
    # 4. æ¨¡å‹æ„å»º
    # =============================
    print("Building model...")
    
    # æ–‡æœ¬ç¼–ç å™¨
    text_encoder = build_module(cfg.text_encoder, MODELS, device=device, dtype=dtype)
    
    # VAEæ¨¡å‹
    vae = build_module(cfg.vae, MODELS).to(device, dtype).eval()
    
    # ä¸»è¦çš„æ‰©æ•£æ¨¡å‹
    input_size = (cfg.num_frames, *cfg.image_size)
    latent_size = vae.get_latent_size(input_size)
    
    model = (
        build_module(
            cfg.model,
            MODELS,
            input_size=latent_size,
            in_channels=vae.out_channels,
            caption_channels=text_encoder.output_dim,
            model_max_length=text_encoder.model_max_length,
            dtype=dtype,
        )
        .to(device, dtype)
        .eval()
    )
    
    print(f"Model parameters: {format_numel_str(get_model_numel(model))}")
    
    # =============================
    # 5. åº”ç”¨æ–¹å·®åˆ†æè¡¥ä¸ ğŸ”¥
    # =============================
    print("\n" + "="*60)
    print("åº”ç”¨xå‚æ•°æ–¹å·®åˆ†æè¡¥ä¸...")
    print("="*60)
    
    # åˆ›å»ºæ–¹å·®åˆ†ææ—¥å¿—ç›®å½•
    variance_log_dir = cfg.get("variance_log_dir", "./variance_logs")
    os.makedirs(variance_log_dir, exist_ok=True)
    
    # åº”ç”¨æ–¹å·®åˆ†æè¡¥ä¸
    model = apply_variance_analysis_patch(
        model, 
        save_log=True, 
        log_dir=variance_log_dir
    )
    
    print("æ–¹å·®åˆ†æè¡¥ä¸åº”ç”¨å®Œæˆï¼")
    print("="*60 + "\n")
    
    # =============================
    # 6. è°ƒåº¦å™¨æ„å»º
    # =============================
    print("Building scheduler...")
    scheduler = build_module(cfg.scheduler, SCHEDULERS)
    
    # =============================
    # 7. CPUå¸è½½é…ç½®
    # =============================
    if cfg.get("cpu_offload", False):
        print("Enabling CPU offload...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ CPUå¸è½½çš„å…·ä½“å®ç°
    
    # =============================
    # 8. æ¨ç†å¾ªç¯
    # =============================
    print("Starting inference...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = cfg.get("output_dir", "./outputs")
    os.makedirs(output_dir, exist_ok=True)
    
    # æ¨ç†å‚æ•°
    num_frames = cfg.get("num_frames", 20)
    height, width = cfg.image_size
    
    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader, desc="Inference")):
            print(f"\n{'='*50}")
            print(f"Processing batch {i+1}/{len(dataloader)}")
            print(f"{'='*50}")
            
            # æ•°æ®é¢„å¤„ç†
            y = batch.pop("captions")[0] if "captions" in batch else [""]
            maps = batch.pop("bev_map_with_aux").to(device, dtype) if "bev_map_with_aux" in batch else None
            bbox = batch.pop("bboxes_3d_data") if "bboxes_3d_data" in batch else None
            cams = batch.pop("cams") if "cams" in batch else None
            rel_pos = batch.pop("rel_pos") if "rel_pos" in batch else None
            
            # ç”Ÿæˆéšæœºå™ªå£°ä½œä¸ºåˆå§‹æ½œåœ¨è¡¨ç¤º
            # è¿™é‡Œçš„zå°±æ˜¯ä¼šä¼ é€’ç»™æ¨¡å‹çš„xå‚æ•°
            z_shape = (1, vae.out_channels * 6, num_frames, height // 8, width // 8)  # 6ä¸ªè§†è§’
            z = torch.randn(z_shape, device=device, dtype=dtype)
            
            print(f"Initial latent z shape: {z.shape}")
            print(f"Initial latent z variance: {torch.var(z).item():.6f}")
            
            # å‡†å¤‡æ¨¡å‹å‚æ•°
            model_args = {
                "maps": maps,
                "bbox": bbox,
                "cams": cams,
                "rel_pos": rel_pos,
                "fps": cfg.get("fps", 8),
                "height": height,
                "width": width,
                "num_frames": num_frames,
            }
            
            # ç§»é™¤Noneå€¼
            model_args = {k: v for k, v in model_args.items() if v is not None}
            
            print(f"Model args keys: {list(model_args.keys())}")
            
            # è°ƒåº¦å™¨é‡‡æ ·
            print("\nStarting scheduler sampling...")
            print("-" * 30)
            
            try:
                samples = scheduler.sample(
                    model,
                    text_encoder,
                    z=z,
                    prompts=y,
                    device=device,
                    additional_args=model_args,
                )
                
                print(f"\nSampling completed! Output shape: {samples.shape}")
                print(f"Output variance: {torch.var(samples).item():.6f}")
                
                # VAEè§£ç 
                print("Decoding with VAE...")
                decoded_samples = vae.decode(samples)
                
                print(f"Decoded samples shape: {decoded_samples.shape}")
                print(f"Decoded samples range: [{decoded_samples.min().item():.3f}, {decoded_samples.max().item():.3f}]")
                
                # ä¿å­˜ç»“æœï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¿å­˜é€»è¾‘ï¼‰
                # save_samples(decoded_samples, output_dir, i)
                
            except Exception as e:
                print(f"Error during sampling: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            print(f"Batch {i+1} completed!")
            
            # é™åˆ¶å¤„ç†çš„æ‰¹æ¬¡æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            if cfg.get("max_batches", None) and i >= cfg.max_batches - 1:
                print(f"Reached maximum batches limit: {cfg.max_batches}")
                break
    
    # =============================
    # 9. ä¿å­˜æ–¹å·®åˆ†ææ—¥å¿—
    # =============================
    print("\n" + "="*50)
    print("ä¿å­˜æ–¹å·®åˆ†ææ—¥å¿—...")
    print("="*50)
    
    if hasattr(model, 'save_variance_log'):
        model.save_variance_log()
        print(f"æ–¹å·®åˆ†ææ—¥å¿—å·²ä¿å­˜åˆ°: {variance_log_dir}")
    
    print("æ¨ç†å®Œæˆï¼")
    print("="*50)


if __name__ == "__main__":
    main()